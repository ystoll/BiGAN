{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, IterableDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "# import import_ipynb\n",
    "from bgan_i_ganOrig import BGAN, BGAN_D, run_on_batch\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "save_path = \"data/saved_models/model.tar\"#vaegan_model - Copy.tar\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if not os.path.exists(\"data/saved_models\"):\n",
    "    os.makedirs(\"data/saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--evalPred'], dest='evalPred', nargs=None, const=None, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARG_PARSER = ArgumentParser()\n",
    "\n",
    "ARG_PARSER.add_argument('--nfeatures', default=609, type=int)\n",
    "ARG_PARSER.add_argument('--dfeatures', default=43, type=int)\n",
    "ARG_PARSER.add_argument('--ehidden', default=300, type=int)\n",
    "ARG_PARSER.add_argument('--model', type=str)\n",
    "\n",
    "ARG_PARSER.add_argument('--ehr', default=False)\n",
    "ARG_PARSER.add_argument('--air', default=True)\n",
    "ARG_PARSER.add_argument('--mimic', default=False)\n",
    "\n",
    "ARG_PARSER.add_argument('--num_epochs', default=200, type=int)\n",
    "ARG_PARSER.add_argument('--seq_len', default=40, type=int)\n",
    "ARG_PARSER.add_argument('--pred_len', default=5, type=int)\n",
    "ARG_PARSER.add_argument('--batch_size', default=200, type=int)\n",
    "ARG_PARSER.add_argument('--missingRate', default=10, type=int)\n",
    "ARG_PARSER.add_argument('--patience', default=20, type=int)\n",
    "ARG_PARSER.add_argument('--e_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--g_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "ARG_PARSER.add_argument('--resume_training', default=False)\n",
    "ARG_PARSER.add_argument('--train', default=True)\n",
    "ARG_PARSER.add_argument('--evalImp', default=False)\n",
    "ARG_PARSER.add_argument('--evalPred', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = ARG_PARSER.parse_args(args=[])\n",
    "MAX_SEQ_LEN = ARGS.seq_len\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, path, chunksize,length,seq_len,flag):\n",
    "        self.path = path\n",
    "        self.chunksize = chunksize\n",
    "        self.len = int(length)#number of times total getitem is called\n",
    "        self.seq_len=seq_len\n",
    "        self.flag=flag\n",
    "        self.reader=pd.read_csv(\n",
    "                self.path,header=0,\n",
    "                chunksize=self.chunksize) # ,names=['data']))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.reader.get_chunk(self.chunksize)\n",
    "        #sex=pd.read_csv('C:\\\\Users/mehak/Desktop/demo.csv',header=0)\n",
    "        #sex=sex[['person_id','Sex']]\n",
    "        #data = pd.merge(data, sex, how='left', on=['person_id'])\n",
    "        #print(data.shape)\n",
    "        #data=data.sort_values(by=['RANDOM_PATIENT_ID','VISIT_YEAR','VISIT_MONTH'])\n",
    "        #print(data['RANDOM_PATIENT_ID'].unique())\n",
    "#         del data['person_id']\n",
    "#         print(data.columns.get_loc('BMI'))\n",
    "        #print(data.columns)\n",
    "\n",
    "        data=data.replace(np.inf,0)\n",
    "        data=data.replace(np.nan,0)\n",
    "        data=data.fillna(0)\n",
    "        #print(data.shape)\n",
    "        if(self.flag==0):\n",
    "#             data['Age']=data['Age'].apply(lambda x: ((x*12)/3)-81)\n",
    "            pids=data['person_id']\n",
    "#             age=data['age']\n",
    "#             del data['age']\n",
    "            pids = T.as_tensor(pids.values.astype(float), dtype=T.long)\n",
    "#             age = T.as_tensor(age.values.astype(float), dtype=T.long)\n",
    "#             print(\"age\",data['Age'])\n",
    "#             print(\"pids\",list(pids))\n",
    "#             print(\"========================================================\")\n",
    "\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "    #         print(list(data[:,0]))\n",
    "    #         print(\"========================================================\")\n",
    "            #data=T.from_numpy(data)\n",
    "            #data=data.double()\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            #print(data.shape)\n",
    "            #print(\"age\",data[0,:,203])\n",
    "            #mask=pd.DataFrame()\n",
    "            #mask = data.loc[data['LABS_LDL_MEAN']>0,'LABS_LDL_MEAN']=1\n",
    "            #df[df['LABS_LDL_MEAN']<0].count()\n",
    "            return data,pids\n",
    "        else:\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            \n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf#11.1179\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, discriminator, optimizer, optimizer_d, save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, discriminator, optimizer, optimizer_d, save_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, discriminator, optimizer, optimizer_d, save_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, discriminator, optimizer, optimizer_d, save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        T.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            'trainer': optimizer.state_dict(),\n",
    "            \"discriminator\": discriminator.state_dict(),\n",
    "            'trainer_d': optimizer_d.state_dict()\n",
    "        }, save_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_test(args, model, discriminator, predWin):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    with T.autograd.no_grad():\n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'test.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'test.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "            \n",
    "                #values to be predicted\n",
    "                y = data.clone().detach()\n",
    "#                 print(y.shape)\n",
    "#                 print(data.shape)\n",
    "                testMask = mask.clone().detach()\n",
    "    #             sex=y[:,:,608]\n",
    "    #             age=y[:,:,607]\n",
    "                #print(sex.shape,age.shape)\n",
    "                y=y[:,:,811]\n",
    "\n",
    "                #------------remove last 5 timestamps------------------\n",
    "                #print(data[0:10,8:,653])\n",
    "                for i in range(data.shape[0]):\n",
    "                    #if(data[i,])\n",
    "                    j=40\n",
    "                    if(predWin==8):\n",
    "                        k=32\n",
    "                    elif(predWin==7):\n",
    "                        k=28\n",
    "                    elif(predWin==6):\n",
    "                        k=24\n",
    "                    elif(predWin==5):\n",
    "                        k=20\n",
    "\n",
    "                    data[i,j-k:j,:]=0\n",
    "                    mask[i,j-k:j]=0\n",
    "                    y[i,0:j-k]=0\n",
    "    #                 age[i,0:j-k]=0\n",
    "    #                 sex[i,0:j-k]=0\n",
    "                    #print(sex.shape,age.shape)\n",
    "                    #yOrig[i,0:j-k]=0\n",
    "                    testMask[i,0:j-k]=0\n",
    "\n",
    "\n",
    "                ret_f, ret,disc = run_on_batch(model,discriminator,data,mask,decay,rdecay, args, optimizer=None,optimizer_d=None,epoch=None)#,bmi_norm)\n",
    "                #print(\"Input\",data.shape)\n",
    "                #print(data[0,:,316])\n",
    "                #print(\"Reverse\",ret['imputations'][0,:])\n",
    "                #print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "                #print(\"Original\",y.shape)\n",
    "                #print(y[0,:])\n",
    "                #print(\"Mask\",testMask.shape)\n",
    "                #print(testMask[0,:])\n",
    "                RLoss=RLoss+ret['loss']\n",
    "                FLoss=FLoss+ret_f['loss']\n",
    "#                 testMask=testMask.cuda()\n",
    "#                 y=y.cuda()\n",
    "                outputBMI=ret['imputations'] * testMask\n",
    "                outputBMIF=ret_f['imputations'] * testMask\n",
    "                mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                #print(\"RMSELoss Revrese: \",mseLoss)\n",
    "                #print(\"RMSELoss Forward: \",mseLossF)\n",
    "                outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "                oBmi.extend(outBmi)\n",
    "                oBmiF.extend(outBmiF)\n",
    "                iBmi.extend(inBmi)\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_test(args, model,discriminator,missingRate):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    samples=0\n",
    "    pids=0\n",
    "    with T.autograd.no_grad():\n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'test.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'test.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "            \n",
    "                #values to be predicted\n",
    "                y = data.clone().detach()\n",
    "#                 print(y.shape)\n",
    "#                 print(data.shape)\n",
    "                testMask = mask.clone().detach()\n",
    "    #             sex=y[:,:,608]\n",
    "    #             age=y[:,:,607]\n",
    "                #print(sex.shape,age.shape)\n",
    "                y=y[:,:,811]\n",
    "            \n",
    "                bmi=811\n",
    "\n",
    "\n",
    "                #------------remove last 5 timestamps------------------\n",
    "                #print(data[0:10,8:,653])\n",
    "                for i in range(data.shape[0]):\n",
    "                    #if(data[i,])\n",
    "                    #mask[i,:].loc[mask[i,:].query('value == 1').sample(frac=.1).index,'value'] = 0\n",
    "                    idxs = torch.nonzero(mask[i,:] == 1)\n",
    "                    samples=samples+list(idxs.size())[0]\n",
    "                    if((missingRate==50) & (list(idxs.size())[0]>4)):\n",
    "                        idxs=random.sample(set(idxs),5)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        data[i,idxs[3],bmi]=0\n",
    "                        data[i,idxs[4],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        mask[i,idxs[4]]=0\n",
    "                        pids=pids + 5\n",
    "                    elif((missingRate>=40) & (list(idxs.size())[0]>3)):\n",
    "                        idxs=random.sample(set(idxs),4)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        data[i,idxs[3],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        pids=pids + 4\n",
    "                    elif((missingRate>=30) & (list(idxs.size())[0]>2)):\n",
    "                        idxs=random.sample(set(idxs),3)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        pids=pids + 3\n",
    "                    elif((missingRate>=20) & (list(idxs.size())[0]>1)):\n",
    "                        idxs=random.sample(set(idxs),2)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        pids=pids + 2\n",
    "                    elif((missingRate>=10) & (list(idxs.size())[0]>0)):\n",
    "                        if (i%2==0):\n",
    "                            idxs=random.sample(set(idxs),1)\n",
    "                            data[i,idxs,bmi]=0\n",
    "                            mask[i,idxs]=0\n",
    "                            pids=pids + 1\n",
    "\n",
    "                    #data[i,idxs,316]=0\n",
    "                    #print(mask[i,:])\n",
    "                    #mask[i,idxs]=0\n",
    "                    #print(mask[i,:])\n",
    "                    testMask[i,:]=testMask[i,:]-mask[i,:]\n",
    "                    #print(testMask[i,:])\n",
    "                    #print(y[i,:])\n",
    "                    y[i,:]=y[i,:]*testMask[i,:]\n",
    "\n",
    "\n",
    "                #print(\"Input Data\",data.shape)\n",
    "                #print(\"Input Mask\",mask.shape)\n",
    "                ret_f,ret,disc = run_on_batch(model,discriminator,data,mask,decay,rdecay, args, optimizer=None,optimizer_d=None,epoch=None)#,bmi_norm)\n",
    "                #print(\"Input\",data.shape)\n",
    "                #print(data[0,:,316])\n",
    "                #print(\"Reverse\",ret['imputations'][0,:])\n",
    "                #print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "                #print(\"Original\",y.shape)\n",
    "                #print(y[0,:])\n",
    "                #print(\"Mask\",testMask.shape)\n",
    "                #print(testMask[0,:])\n",
    "                RLoss=RLoss+ret['loss']\n",
    "                FLoss=FLoss+ret_f['loss']\n",
    "    #             testMask=testMask.cuda()\n",
    "    #             y=y.cuda()\n",
    "                #print(\"Output\",ret['imputations'].shape)\n",
    "                #print(\"Output\",ret_f['imputations'].shape)\n",
    "                #print(\"Output mask\",testMask.shape)\n",
    "                outputBMI=ret['imputations'] * testMask\n",
    "                outputBMIF=ret_f['imputations'] * testMask\n",
    "                #print(\"outputBMI\",outputBMI.shape)\n",
    "                #print(outputBMI[0])\n",
    "                #print(\"outputBMIF\",outputBMIF.shape)\n",
    "                #print(outputBMIF[0])\n",
    "                mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                #print(\"RMSELoss Revrese: \",mseLoss)\n",
    "                #print(\"RMSELoss Forward: \",mseLossF)\n",
    "                outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "                oBmi.extend(outBmi)\n",
    "                oBmiF.extend(outBmiF)\n",
    "                iBmi.extend(inBmi)\n",
    "\n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    print(\"Total BMI values\",samples)\n",
    "    print(\"Deleted BMIs\",pids)\n",
    "    print(\"Missing%\",pids/samples)\n",
    "    #print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBmi(outBmi,outBmiF, inBmi, testMask):\n",
    "    \n",
    "    outBmi = outBmi.cpu().detach().numpy()\n",
    "    outBmiF = outBmiF.cpu().detach().numpy()\n",
    "    inBmi = inBmi.cpu().detach().numpy()\n",
    "    testMask = testMask.cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "    outBmi=outBmi[np.nonzero(testMask)]\n",
    "    outBmiF=outBmiF[np.nonzero(testMask)]\n",
    "    inBmi=inBmi[np.nonzero(testMask)]\n",
    "    \n",
    "    \n",
    "    return outBmi,outBmiF,inBmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evalFull(args, model, discriminator):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    GLoss=0\n",
    "    DLoss=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "   \n",
    "    with T.autograd.no_grad():\n",
    "        \n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'val.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'val.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                pids=data[1]\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "                #print(\"Data:\",data[0,0,:])\n",
    "                #print(decay.shape)\n",
    "\n",
    "                ret_f, ret, disc = run_on_batch(model,discriminator,data,mask,decay,rdecay, args, optimizer=None,optimizer_d=None,epoch=None)#,bmi_norm)\n",
    "                RLoss=RLoss+ret['loss']\n",
    "                GLoss=GLoss+disc['loss_g'].item()\n",
    "                DLoss=DLoss+disc['loss_d'].item()\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        GLoss=GLoss/TBatches\n",
    "        DLoss=DLoss/TBatches\n",
    "    #print(\"===================================\")\n",
    "    print(\"Val R Loss:\",RLoss)\n",
    "    print(\"Val G Loss:\",GLoss)\n",
    "    print(\"Val D Loss:\",DLoss)\n",
    "    #print(outputBMI)\n",
    "    return RLoss,GLoss,DLoss\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(args, model, discriminator):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "\n",
    "    trainLoss=[]\n",
    "    discLoss=[]\n",
    "    gLoss=[]\n",
    "    valLoss=[]\n",
    "    gValLoss=[]\n",
    "    discValLoss=[]\n",
    "    \n",
    "    #define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr = 1e-2)\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        optimizer_d.load_state_dict(checkpoint['trainer_d'])\n",
    "        early_stopping(4.2360, model, discriminator, optimizer, optimizer_d, save_path)\n",
    "    \n",
    "    #for every epoch\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "    \n",
    "        #Running Losses\n",
    "        RLoss=0\n",
    "        GLoss=0\n",
    "        DLoss=0\n",
    "        TBatches=0  \n",
    "        print(\"=============EPOCH=================\")\n",
    "        for i in ['M','F']:\n",
    "       \n",
    "            files = 'cond' + i + 'train.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+ G + 'train.csv'\n",
    "\n",
    "            maskFiles = 'mask' + i + 'train.csv'\n",
    "\n",
    "            #print(files)\n",
    "            #print(maskFiles)\n",
    "            #print(\"====================New File========================\")\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                pids=data[1]\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "    #             if args.female:\n",
    "    #                 data=data[:,:,:,1:229]#Female = 1:229, Male = 1:334] \n",
    "\n",
    "    #             if args.male:\n",
    "    #                 data=data[:,:,:,1:611]#Female = 1:229, Male = 1:334]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]    \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "#                 print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "#                 print(\"Data:\",data.shape)\n",
    "                #print(decay.shape)\n",
    "\n",
    "                ret_f, ret, disc = run_on_batch(model,discriminator,data,mask,decay,rdecay, args, optimizer,optimizer_d,epoch)#,bmi_norm)\n",
    "                RLoss=RLoss+ret['loss'].item()\n",
    "                GLoss=GLoss+disc['loss_g'].item()\n",
    "                DLoss=DLoss+disc['loss_d'].item()\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])   \n",
    "\n",
    "            TBatches=TBatches+batch_idx+1\n",
    "            #print(TBatches)\n",
    "            #print(\"File:\", i, \"loss_R:\", \"%.4f\"%RLoss/(batch_idx+1), \"loss_G:\", \"%.4f\"%GLoss/(batch_idx+1), \"loss_D:\", \"%.4f\"%DLoss/(batch_idx+1))\n",
    "                #print(len(encoded))\n",
    "        RLoss=RLoss/TBatches\n",
    "        GLoss=GLoss/TBatches\n",
    "        DLoss=DLoss/TBatches\n",
    "        \n",
    "        print(\"EPOCH:\", epoch, \"loss_R:\", \"%.4f\"%RLoss, \"loss_G:\", \"%.4f\"%GLoss, \"loss_D:\", \"%.4f\"%DLoss)\n",
    "        \n",
    "        trainLoss.append(RLoss)\n",
    "        discLoss.append(DLoss)\n",
    "        gLoss.append(GLoss)\n",
    "\n",
    "        valid_loss,g_valloss,disc_loss = run_evalFull(args, model,discriminator)\n",
    "        \n",
    "        valLoss.append(valid_loss)\n",
    "        discValLoss.append(disc_loss)\n",
    "        gValLoss.append(g_valloss)\n",
    "        #plotBmi(outBmi , inBmi)\n",
    "\n",
    "        \n",
    "        #if epoch<1 or epoch >5:\n",
    "        if not (T.isnan(valid_loss)):\n",
    "            early_stopping(valid_loss, model, discriminator, optimizer, optimizer_d, save_path)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        #plot_grad_flow(model['e'].named_parameters())\n",
    "        #plot_grad_flow(model['g'].named_parameters())\n",
    "        #plot_grad_flow(model['d'].named_parameters())\n",
    "    return trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    \n",
    "    train_on_gpu = T.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "        \n",
    "    model = BGAN(args)\n",
    "    discriminator=BGAN_D()\n",
    "    \n",
    "    # print(\"discriminator\", discriminator)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        discriminator=discriminator.cuda()\n",
    "\n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "        #optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        #optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss = run_epoch(args, model, discriminator) \n",
    "        return trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss\n",
    "    \n",
    "    elif args.train:\n",
    "        trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss = run_epoch(args, model, discriminator) \n",
    "        return trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss\n",
    "        \n",
    "    elif args.evalImp:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        optimizer_d = optim.Adam(discriminator.parameters(), lr = 1e-3)\n",
    "        optimizer_d.load_state_dict(checkpoint['trainer_d'])\n",
    "        #RLoss,GLoss,DLoss = run_evalFull(args, model, discriminator)\n",
    "        oBmi, oBmiF, iBmi = imputation_test(args, model,discriminator,args.missingRate)\n",
    "        \n",
    "    elif args.evalImp:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        optimizer_d = optim.Adam(discriminator.parameters(), lr = 1e-3)\n",
    "        optimizer_d.load_state_dict(checkpoint['trainer_d'])\n",
    "        #RLoss,GLoss,DLoss = run_evalFull(args, model, discriminator)\n",
    "        oBmi, oBmiF, iBmi = pred_test(args, model,discriminator,args.pred_len)\n",
    "        \n",
    "        #return oBmi, oBmiF, iBmi, oAge,oSex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n",
      "=============EPOCH=================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'condMtrain.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-545a7a65a16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#RLoss,GLoss,DLoss = run(ARGS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#oBmi, oBmiF, iBmi, oAge, oSex = run(ARGS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-23abfc8145d5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscValLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgValLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscValLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgValLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ba2b8a494ffc>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(args, model, discriminator)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#print(maskFiles)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#print(\"====================New File========================\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1356100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mmaskDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaskFiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1356100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d014f4279adb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, chunksize, length, seq_len, flag)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         self.reader=pd.read_csv(\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 chunksize=self.chunksize) # ,names=['data']))\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BiGAN/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'condMtrain.csv'"
     ]
    }
   ],
   "source": [
    "#trainLoss,discLoss,gLoss, valLoss,discValLoss,gValLoss = run(ARGS)\n",
    "#RLoss,GLoss,DLoss = run(ARGS)\n",
    "#oBmi, oBmiF, iBmi, oAge, oSex = run(ARGS)\n",
    "run(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a22af5604703639258a51706230e7bc556e9a858939154b7bea874e1a460cea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('BiGAN': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}